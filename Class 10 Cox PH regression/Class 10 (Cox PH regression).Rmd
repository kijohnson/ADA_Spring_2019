---
title: "Cox Proportional Hazards Regression"
author: "Kim Johnson"
date: "3/27/2019"
output:
  slidy_presentation: default
  html_document:
    df_print: paged
  html_notebook: default
  beamer_presentation: default
  pdf_document: default
  ioslides_presentation: default
---

###Outline

1. Overview
2. The partial likelihood estimator
3. Model fitting and significance testing
4. Handling tied survival times
5. Interpreting the estimate model using HRs
6. Interpeting the estimated model using model-based survival curves

###Cox model

![](Cox model specification.png)

###Key features

- No requirement for underlying distribution of survival times
- Assumes a constant ratio of hazards between any two individuals (i.e. proportional hazards)

![](Cox model 2.png)

- The model assumes that the hazard for any individual is a fixed proportion of the hazard for any other individual
- Graphically, the hazard functions for any two individuals should be strictly parallel
- What's important is that h$_{0}$(t), the baseline hazard rate, cancels out of the numerator and denominator. This means that you can estimate the $\beta$ coefficients of the model without having to specify the baseline hazard rate
- Violation of the assumption of PH is not a serious problem (Dr. Guo and Paul Allison)

###Partial likelihood estimation

- Because h$_{0}$(t) cancels out, Cox developed a method called partial likelihood estimation to estimate the $\beta$'s, which discards the baseline hazard and only treats the second part of the equation as though it were an ordinary likelihood function.
- It is called partial likelihood because the formula only includes probabilities for subjects who fail and not for those who are censored (although they are considered in the risk set)

###Partial likelihood demo

- Let's see how partial likelihood (PL) estimation works. The goals of this exercise are 1) to understand Cox regression and PL; and 2) to get a feel for the general procedure of ML estimation

- Three steps for MLE:
1. Develop a likelihood function (which maximizes the likelihood that the model can reproduce your sample)
2. Plug in starting values for $\beta$'s
3. Using a numeric approach to test different sets of $\beta$'s (iteration, convergence, and convergence criterion)

###Partial likelihood estimation (only for single parameter no ties data)

-**Step 1:** Develop PL function
- Sort data in ascending order of T (survival times)
- The hazard for individual 1 is:

![](Cox model 3.png) 

### Partial likelihood estimation continued

- The likelihood for individual 1 to have the event at time t is the "hazard for individual 1 at time t/(hazard for the risk set at time t)" or 
![](PL1.png)

- The likelihood is equal to the hazard for case 1 at time t divided by the sum of the hazards for all cases who were at risk of having the event at time t.  

###Partial likelihood estimation continued
- Baseline hazard cancels out
- As a result, the likelihood function is completely expressed by $\beta$$_{x}$-the coefficient to be determined by the predictor
- The model carefully takes the information of censoring of cases into consideration when building the likelihood function - censoring case information is built into the construction of the risk set.

###Partial likelihood estimation continued

- The total likelihood function for the sample is then written as a product of the likelihoods for all cases in the sample:

![](PL2.png) 

and defines the likelihood for a censoring case (e.g. case 21, dead=0) as 

![](L1.png) 


###Partial likelihood estimation continued
- Putting together the formula more formally

![](PL3.png)


- Where n = number of failure times
- Where Y$_{ij}$=1 if t$_{j}$$\ge$t$_{i}$; and Y$_{ij}$ = 0 if t$_{j}$$\lt$t$_{i}$. Here $\delta$$_{i}$ is the censoring indicator and serves like an on/off switch. Again the likelihood function for censoring cases is "no event" or "excluded" (multiply by 1), but the information is taken into consideration in the denominator or risk set.

###Partial likelihood estimation continued

- It is convenient to take the logarithm of the PL, and maximize log PL:

![](PL4.png)

-**Step 2**: Plug in starting value of $\beta$

-**Step 3**: Search the best $\beta$ until you maximize the log PL

- Let's see how it works in excel

###Partial likelihood estimation continued

- The excel program was designed to maximize the following function:

![](PL4.png)

![](IMG_3897.jpg){width=500px}

- An important note: It only works with one-predictor-no-ties data

###Model fitting and significance testing

- To determine whether an estimated Cox model fits the data to an acceptable degree, we can use the likelihood ratio test:

- G=2[L$_{p}$($\widehat{\beta}$)-L$_{p}$(0)]

where L$_{p}$($\widehat{\beta}$) denotes the log likelihood of the model containing all covariates, and L$_{p}$(0) the log likelikhood evaluated at all $\beta$'s = 0
- the test statistic *G* is subject to a chi-square distribution with a given degree of freedom (i.e., the number of independent variables included)

###Model fitting and sigificance testing continued

- There are three methods to asess the significance of the estimated coefficient:
1. The Wald test (recommended):

![](Z1.png)

- Using z*, we can perform a two-tailed or one-tailed z test.

2. The partial likelihood ratio test using: *G*=2[L$_{p}$(M1)-L$_{p}$(M2)]

where L$_{p}$(M1) is the log likelihood of the model containing the covariates being tested and L$_{p}$(M2) is the log likelihood of the reduced model without the test covariates. Using *G* to perform a chi-square test with *df=p* where *p* is the number of test covariates

###Model fitting and sigificance testing continued

3. The score test:

- The test statistic is the ratio of the derivative of the log partial likelihood to the square root of the observed information all evaluated at $\beta$ = 0:

![](score_test.png)

- Using z* we perform a two-tailed or one-tailed z test.
- Note Stata does not provide the score test. SAS and SPSS report square of z* that follows a chi-square distribution. In practice, results from the Wald and score tests are very close.

###Handling tied survival times

- When some cases are tied on event times (i.e., two or more cases whose events occur at the same time), you need to choose an appropriate method to handle ties
- General ideas: consider true time ordering for the tied cases. Four methods:
    - **Breslow** (default in Stata & SAS): assumes that the observed ties occurred sequentially.
    - **Efron**: consider all possible underlying orderings that might exist, but use a numeric approximation to simplify computations
    - **Exact**: consider all possible orderings (time-consuming: 5! = 120 possible orderings)
    - **Discrete**: assumes time is really discrete.

###Handling tied survival times continued

- How to choose a method to handle ties? Efron is the default for R and highly recommended
- Computing time is not a concern so exact methods can always produce good results

###Hazard ratio interpretations

- The measure of association from a Cox model is called the hazard ratio (HR)
- HR=exp($\widehat{\beta}$)
- The confidence interval 100(1-$\alpha$)%

![](CI.png)

- 95% confidence interval

![](CI2.png)

- A 95% CI that does not include the value 1 is an indicator that the HR is significant at the 0.05 level

###Hazard ratio interpretations continued

- Examples of interpretations based on HR from Allison, 1995
- Continuous variable: $\beta$(age) = -.057246, exp($\beta$)=.944, 1-.944=.056 “for each one-year increase in the age at release, the hazard of arrest goes down by an estimated 5.6%”;
- Dummy variable: $\beta$(financial aid) = -.379022, exp($\beta$)=.685, “the hazard of arrest for those who received financial aid is only about 69% of the hazard for those who did not received aid, controlling for all other variables”.  Alternatively you can compare exp($\beta$) with value 1 to do a 1-exp($\beta$) or exp($\beta$)-1 to talk about the hazard rate for the group that is X% lower (or X% higher) than that of the reference group. That is, do 1-.685=.315, and say “the hazard of arrest for those who received aid is 31.5% lower than those who did not.” This is similar to the interpretation of using odds ratio for a logistic regression.


###Hazard ratio interpretations continued

- For a continuous variable, sometimes you may want to interpret the change on hazard rate by using c-units rather than one-unit on the covariate. That is, you want to use clinically interesting unit of change. For example, you may talk about the relative hazard rate for every 5-year increase in age rather than a one-year increase. See Hosmer et al, pp.106-108. 

![](cunit.png)

- Example: 

![](cunit_example.png)

- Every 5-year increase in age increases the hazard rate by 26% with a 95% CI of 12-42%

###Go to R code to run some models.

###Model-predicted survivor curves

- Sometimes we may want to show adjusted survival curves (rather than bivariate KM curves)
- The key idea behind  model-predicted survivor curves: "Other things being equal (or controlling for all other covariates), what is going to happen?"
- How to control for all other covariates? Three methods: 1) Control them at the sample mean; 2) using quantiles (percentiles) of the risk scores to show selected survival curves and 3) using the median of the modifed risk scores

###Model-predicted survivor curves continued
- **Method 1**: Controlling other covariates at the sample means, we have:

![](MPcurves.png)

- where S$_{0}$(t) is the baseline survivor function, and i=1,2,..n, or n survivor curves of interest. You allow x$_{4}$ to change values n times.
- You can use any combination of covariate values of interest
- After estimating $\beta$ by partial likelihood, you can get an estimate of S$_{0}$(t) by a nonparametric maximum likelihood method. With estimate in hand (SAS does this internally when you specify BASELINE), you can generate the model-predicted survivor function for any set of covariate values by substitution in the equation above.

###Model-predicted survivor curves continued
- **Method 2**: Use the percentiles of the risk scores to show selected survival curves:

![](MPcurves2.png)

where $\widehat{r}$$_{q}$ is the percentile value of the risk scores of the sample:

![](rq.png)

You choose 10th, 25th, 50th, 75th, and 90th percentile of the risk scores from the sample, and follow the same procedure to generate the model-predicted survivor curves.

###Model-predicted survivor curves continued

**Method 3**: use the median of the modified risk scores, we have:

![](rm1.png)

where $\widehat{r}$m$_{50}$ is the median value of the modified risk scores of the sample:

![](rm4.png)

You use different values of x$_{4}$ to get different predicted survivor functions. You follow the same procedure to generate the model-predicted survivor curves.

###Go to R code to run some survival curves

###PH assumption testing
- PH assumption: HR is constant over time
- Three methods for testing the PH assumption:
1. Graphical
2. goodness-of-fit tests
3. time-dependent variables

###What to do if the assumption is violated?
1. Nothing not a big deal
2. Stratified Cox procedure
    - splits sample into subsamples on levels of stratification variable
    - assumes different baseline hazards
    - get different survival curves for each strata but one HR
    - used for adjusting variable and not main variable of interest
3. Report hazard ratios for varying follow-up intervals
  
###Go to R code to test PH assumption